{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.sparse import csr_matrix\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "subset10k = pd.read_csv(\"../raw_data/track_meta_milestone3.csv\", index_col=[\"Unnamed: 0\"])\n",
    "# subset100 = pd.read_csv(\"../raw_data/track_meta_100subset_new.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-val-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "subset10k_id = np.random.choice(subset10k['Playlistid'].unique(), size = 10000, replace = False)\n",
    "subset10k = subset10k[subset10k['Playlistid'].isin(subset10k_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-val-test split (20%)\n",
    "train, test = train_test_split(subset10k, test_size=0.2, random_state=42, stratify = subset10k['Playlistid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Playlistid</th>\n",
       "      <th>Trackid</th>\n",
       "      <th>Artist_Name</th>\n",
       "      <th>Track_Name</th>\n",
       "      <th>Album_Name</th>\n",
       "      <th>Track_Duration</th>\n",
       "      <th>Artist_uri</th>\n",
       "      <th>Track_uri</th>\n",
       "      <th>Album_uri</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>...</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>tempo</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>valence</th>\n",
       "      <th>Playlist</th>\n",
       "      <th>Album</th>\n",
       "      <th>Track</th>\n",
       "      <th>Artist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>193947</th>\n",
       "      <td>2914</td>\n",
       "      <td>11</td>\n",
       "      <td>El Fantasma</td>\n",
       "      <td>Vengo a Aclarar (En Vivo)</td>\n",
       "      <td>Vengo a Aclarar (En Vivo)</td>\n",
       "      <td>156591</td>\n",
       "      <td>spotify:artist:0my6Pg4I28dVcZLSpAkqhv</td>\n",
       "      <td>spotify:track:2S6sWOPw6dDDqZwSbKop1y</td>\n",
       "      <td>spotify:album:5VAMpS1xlpm9FOh4jvhbHW</td>\n",
       "      <td>0.61700</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.007</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1090</td>\n",
       "      <td>115.348</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.8660</td>\n",
       "      <td>new</td>\n",
       "      <td>30</td>\n",
       "      <td>35</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6971102</th>\n",
       "      <td>104929</td>\n",
       "      <td>1</td>\n",
       "      <td>Lil Uzi Vert</td>\n",
       "      <td>XO TOUR Llif3</td>\n",
       "      <td>Luv Is Rage 2</td>\n",
       "      <td>182706</td>\n",
       "      <td>spotify:artist:4O15NlyKLIASxsJ0PrXPfz</td>\n",
       "      <td>spotify:track:7GX5flRQZVHRAGd6B4TmDO</td>\n",
       "      <td>spotify:album:733e1ZfktLSwj96X5rsMeE</td>\n",
       "      <td>0.00264</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.366</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2310</td>\n",
       "      <td>155.096</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.4010</td>\n",
       "      <td>bop</td>\n",
       "      <td>16</td>\n",
       "      <td>26</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8881417</th>\n",
       "      <td>133478</td>\n",
       "      <td>7</td>\n",
       "      <td>James Bay</td>\n",
       "      <td>Let It Go</td>\n",
       "      <td>Chaos And The Calm</td>\n",
       "      <td>260533</td>\n",
       "      <td>spotify:artist:4EzkuveR9pLvDVFNx6foYD</td>\n",
       "      <td>spotify:track:13HVjjWUZFaWilh2QUJKsP</td>\n",
       "      <td>spotify:album:5BxvswQSGWrBbVCdx6mFGO</td>\n",
       "      <td>0.81800</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.396</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0288</td>\n",
       "      <td>147.464</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.2460</td>\n",
       "      <td>Thinking Out Loud</td>\n",
       "      <td>17</td>\n",
       "      <td>22</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122724</th>\n",
       "      <td>1827</td>\n",
       "      <td>165</td>\n",
       "      <td>The Weeknd</td>\n",
       "      <td>As You Are</td>\n",
       "      <td>Beauty Behind The Madness</td>\n",
       "      <td>340493</td>\n",
       "      <td>spotify:artist:1Xyo4u8uXC1ZmMpatF05PJ</td>\n",
       "      <td>spotify:track:5hmO72DpMfddRFBbVdPoEX</td>\n",
       "      <td>spotify:album:28ZKQMoNBB0etKXZ97G2SN</td>\n",
       "      <td>0.14500</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.835</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0526</td>\n",
       "      <td>173.664</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0554</td>\n",
       "      <td>Now</td>\n",
       "      <td>52</td>\n",
       "      <td>222</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4336242</th>\n",
       "      <td>65235</td>\n",
       "      <td>78</td>\n",
       "      <td>Big Sean</td>\n",
       "      <td>Dance (A$$)</td>\n",
       "      <td>Finally Famous</td>\n",
       "      <td>197760</td>\n",
       "      <td>spotify:artist:0c173mlxpT3dSFRgMO8XPh</td>\n",
       "      <td>spotify:track:67W5fd1ld7dqHNCfaQu52I</td>\n",
       "      <td>spotify:album:3jQol8uDHmZIgqaVGogf37</td>\n",
       "      <td>0.02000</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.569</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3420</td>\n",
       "      <td>159.865</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.7000</td>\n",
       "      <td>tbt</td>\n",
       "      <td>79</td>\n",
       "      <td>84</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Playlistid  Trackid   Artist_Name                 Track_Name  \\\n",
       "193947         2914       11   El Fantasma  Vengo a Aclarar (En Vivo)   \n",
       "6971102      104929        1  Lil Uzi Vert              XO TOUR Llif3   \n",
       "8881417      133478        7     James Bay                  Let It Go   \n",
       "122724         1827      165    The Weeknd                 As You Are   \n",
       "4336242       65235       78      Big Sean                Dance (A$$)   \n",
       "\n",
       "                        Album_Name  Track_Duration  \\\n",
       "193947   Vengo a Aclarar (En Vivo)          156591   \n",
       "6971102              Luv Is Rage 2          182706   \n",
       "8881417         Chaos And The Calm          260533   \n",
       "122724   Beauty Behind The Madness          340493   \n",
       "4336242             Finally Famous          197760   \n",
       "\n",
       "                                    Artist_uri  \\\n",
       "193947   spotify:artist:0my6Pg4I28dVcZLSpAkqhv   \n",
       "6971102  spotify:artist:4O15NlyKLIASxsJ0PrXPfz   \n",
       "8881417  spotify:artist:4EzkuveR9pLvDVFNx6foYD   \n",
       "122724   spotify:artist:1Xyo4u8uXC1ZmMpatF05PJ   \n",
       "4336242  spotify:artist:0c173mlxpT3dSFRgMO8XPh   \n",
       "\n",
       "                                    Track_uri  \\\n",
       "193947   spotify:track:2S6sWOPw6dDDqZwSbKop1y   \n",
       "6971102  spotify:track:7GX5flRQZVHRAGd6B4TmDO   \n",
       "8881417  spotify:track:13HVjjWUZFaWilh2QUJKsP   \n",
       "122724   spotify:track:5hmO72DpMfddRFBbVdPoEX   \n",
       "4336242  spotify:track:67W5fd1ld7dqHNCfaQu52I   \n",
       "\n",
       "                                    Album_uri  acousticness   ...   loudness  \\\n",
       "193947   spotify:album:5VAMpS1xlpm9FOh4jvhbHW       0.61700   ...     -6.007   \n",
       "6971102  spotify:album:733e1ZfktLSwj96X5rsMeE       0.00264   ...     -6.366   \n",
       "8881417  spotify:album:5BxvswQSGWrBbVCdx6mFGO       0.81800   ...    -10.396   \n",
       "122724   spotify:album:28ZKQMoNBB0etKXZ97G2SN       0.14500   ...     -9.835   \n",
       "4336242  spotify:album:3jQol8uDHmZIgqaVGogf37       0.02000   ...     -6.569   \n",
       "\n",
       "         mode  speechiness    tempo  time_signature  valence  \\\n",
       "193947    1.0       0.1090  115.348             3.0   0.8660   \n",
       "6971102   0.0       0.2310  155.096             4.0   0.4010   \n",
       "8881417   1.0       0.0288  147.464             4.0   0.2460   \n",
       "122724    0.0       0.0526  173.664             4.0   0.0554   \n",
       "4336242   1.0       0.3420  159.865             4.0   0.7000   \n",
       "\n",
       "                  Playlist  Album  Track  Artist  \n",
       "193947                 new     30     35      26  \n",
       "6971102                bop     16     26      12  \n",
       "8881417  Thinking Out Loud     17     22      11  \n",
       "122724                 Now     52    222      44  \n",
       "4336242                tbt     79     84      65  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create co-occurence matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Binary Sparse Matrix\n",
    "co_mat = pd.crosstab(train.Playlistid, train.Track_uri)\n",
    "co_mat = co_mat.clip(upper=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALS Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecommenderSystem():\n",
    "    \"\"\"Represents the scheme for implementing a recommender system.\"\"\"\n",
    "    def __init__(self, training_data, *params):\n",
    "        \"\"\"Initializes the recommender system.\n",
    "\n",
    "        Note that training data has to be provided when instantiating.\n",
    "        Optional parameters are passed to the underlying system.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def train(self, *params):\n",
    "        \"\"\"Starts training. Passes optional training parameters to the system.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def score(self, user_id, item_id):\n",
    "        \"\"\"Returns a single score for a user-item pair.\n",
    "\n",
    "        If no prediction for the given pair can be made, an exception should be raised.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "class ALSRecommenderSystem(RecommenderSystem):\n",
    "    \"\"\"Provides a biased ALS-based implementation of an implicit recommender system.\"\"\"\n",
    "    def __init__(self, training_data, biased, latent_dimension, log_dir=None, confidence=20):\n",
    "        \"\"\"Initializes the recommender system.\n",
    "\n",
    "        Keyword arguments:\n",
    "        training_data: Data to train on.\n",
    "        biased: Whether to include user- and item-related biases in the model.\n",
    "        latent_dimension: Dimension of the latent space.\n",
    "        log_dir: Optional pointer to directory storing logging information.\n",
    "        confidence: Confidence value that should be assigned to pairs where interaction\n",
    "                    was present. Since the data includes single interactions only, simply\n",
    "                    assigining 1 for non-interactions and this value otherwise suffices.\n",
    "                    Should be greater than 1.\n",
    "        \"\"\"\n",
    "        self.biased = biased\n",
    "        self.confidence = confidence\n",
    "        self.latent_dimension = latent_dimension\n",
    "        self.U = None\n",
    "        self.V = None\n",
    "        self.log_dir = log_dir\n",
    "        self.C_users, self.P_users, self.C_items, self.P_items, self.mapping_users, self.mapping_items = self._build_matrices(training_data, confidence)\n",
    "        self.user_dim, self.item_dim = self.P_users.shape\n",
    "\n",
    "    def _build_matrices(self, activity, confidence):\n",
    "        \"\"\"Build the initial matrices.\"\"\"\n",
    "        distinct_users = len(set(activity['user']))\n",
    "        distinct_items = len(set(activity['items']))\n",
    "        C_users = np.ones(shape=(distinct_users, distinct_items))\n",
    "        P_users = np.zeros(shape=(distinct_users, distinct_items))\n",
    "        C_items = np.ones(shape=(distinct_items, distinct_users))\n",
    "        P_items = np.zeros(shape=(distinct_items, distinct_users))\n",
    "\n",
    "        mapping_users = {}\n",
    "        mapping_items = {}\n",
    "        user_ct = 0\n",
    "        items_ct = 0\n",
    "\n",
    "        for index, row in activity.iterrows():\n",
    "            user, items = row\n",
    "            if not user in mapping_users:\n",
    "                mapping_users[user] = user_ct\n",
    "                user_ct += 1\n",
    "            if not items in mapping_items:\n",
    "                mapping_items[items] = items_ct\n",
    "                items_ct += 1\n",
    "            user_index, items_index = mapping_users[user], mapping_items[items]\n",
    "            C_users[user_index, items_index] = confidence\n",
    "            P_users[user_index, items_index] = 1\n",
    "            C_items[items_index, user_index] = confidence\n",
    "            P_items[items_index, user_index] = 1\n",
    "        return C_users, P_users, C_items, P_items, mapping_users, mapping_items\n",
    "\n",
    "    def save(self, directory):\n",
    "        \"\"\"Saves current matrices to the given directory.\"\"\"\n",
    "        np.save(os.path.join(directory, 'U.npy'), self.U)\n",
    "        np.save(os.path.join(directory, 'V.npy'), self.V)\n",
    "        #np.save(os.path.join(directory, 'training_data.npy'), self.training_data)\n",
    "        np.save(os.path.join(directory, 'params.npy'), np.array([self.confidence]))\n",
    "        if self.biased:\n",
    "            np.save(os.path.join(directory, 'user_biases.npy'), self.user_biases)\n",
    "            np.save(os.path.join(directory, 'item_biases.npy'), self.item_biases)\n",
    "\n",
    "    def load(self, directory):\n",
    "        \"\"\"Loads matrices from the given directory.\"\"\"\n",
    "        self.U = np.load(os.path.join(directory, 'U.npy'))\n",
    "        self.V = np.load(os.path.join(directory, 'V.npy'))\n",
    "        self.training_data = np.load(os.path.join(directory, 'training_data.npy'))\n",
    "        self.confidence = np.load(os.path.join(directory, 'params.npy')).flatten()\n",
    "        if self.biased:\n",
    "            self.user_biases = np.load(os.path.join(directory, 'user_biases.npy'))\n",
    "            self.item_biases = np.load(os.path.join(directory, 'item_biases.npy'))\n",
    "\n",
    "        self.C_users, self.P_users, self.C_items, self.P_items, self.mapping_users, self.mapping_items = self._build_matrices(self.training_data, self.confidence)\n",
    "        self.user_dim, self.item_dim = self.P_users.shape\n",
    "\n",
    "    def _single_step(self, lbd):\n",
    "        \"\"\"Executes a single optimization step using (biased) ALS, with lbd as regularization factor.\"\"\"\n",
    "        C_users, P_users, C_items, P_items, mapping_users, mapping_items = self.C_users, self.P_users, self.C_items, self.P_items, self.mapping_users, self.mapping_items\n",
    "        biased = self.biased\n",
    "\n",
    "        # Update U.\n",
    "        if biased: # Expand matrices to account for biases.\n",
    "            U_exp = np.hstack((self.user_biases.reshape(-1,1), self.U))\n",
    "            V_exp = np.hstack((np.ones_like(self.item_biases).reshape(-1,1), self.V))\n",
    "            kdim = self.latent_dimension + 1\n",
    "        else: # We work with copies here to make it safer to abort within updates.\n",
    "            U_exp = self.U.copy()\n",
    "            V_exp = self.V.copy()\n",
    "            kdim = self.latent_dimension\n",
    "        Vt = np.dot(np.transpose(V_exp), V_exp)\n",
    "        for user_index in tqdm(range(self.user_dim)):\n",
    "            C = np.diag(C_users[user_index])\n",
    "            d = np.dot(C, P_users[user_index] - (0 if not biased else self.item_biases))\n",
    "            val = np.dot(np.linalg.inv(Vt + np.dot(np.dot(V_exp.T, C - np.eye(self.item_dim)), V_exp) + lbd*np.eye(kdim)), np.transpose(V_exp))\n",
    "            U_exp[user_index] = np.dot(val, d)\n",
    "        if biased:\n",
    "            self.user_biases = U_exp[:,0]\n",
    "            self.U = U_exp[:,1:]\n",
    "        else:\n",
    "            self.U = U_exp\n",
    "\n",
    "        # Update V.\n",
    "        if biased:\n",
    "            U_exp = np.hstack((np.ones_like(self.user_biases).reshape(-1,1), self.U))\n",
    "            V_exp = np.hstack((self.item_biases.reshape(-1,1), self.V))\n",
    "        else: # We work with copies here to make it safer to abort within updates.\n",
    "            U_exp = self.U.copy()\n",
    "            V_exp = self.V.copy()\n",
    "\n",
    "        Ut = np.dot(np.transpose(U_exp), U_exp)\n",
    "        for item_index in tqdm(range(self.item_dim)):\n",
    "            C = np.diag(C_items[item_index])\n",
    "            d = np.dot(C, P_items[item_index] - (0 if not biased else self.user_biases))\n",
    "            val = np.dot(np.linalg.inv(Ut + np.dot(np.dot(U_exp.T, C-np.eye(self.user_dim)), U_exp) + lbd*np.eye(kdim)), np.transpose(U_exp))\n",
    "            V_exp[item_index] = np.dot(val, d)\n",
    "        if biased:\n",
    "            self.item_biases = V_exp[:, 0]\n",
    "            self.V = V_exp[:,1:]\n",
    "        else:\n",
    "            self.V = V_exp\n",
    "\n",
    "    def compute_loss(self, lbd):\n",
    "        \"\"\"Computes loss value on the training data.\n",
    "\n",
    "        Returns a tuple of total loss and prediction loss (excluding regularization loss).\n",
    "        \"\"\"\n",
    "        C_users, P_users, C_items, P_items, mapping_users, mapping_items = self.C_users, self.P_users, self.C_items, self.P_items, self.mapping_users, self.mapping_items\n",
    "        main_loss = 0\n",
    "        # Main loss term.\n",
    "        for user_index in range(self.user_dim):\n",
    "            for item_index in range(self.item_dim):\n",
    "                pred = np.dot(self.U[user_index].T, self.V[item_index])\n",
    "                if self.biased:\n",
    "                    pred += self.user_biases[user_index] + self.item_biases[item_index]\n",
    "                loss = self.C_users[user_index, item_index] * (P_users[user_index, item_index]-pred)**2\n",
    "                main_loss += loss\n",
    "\n",
    "        # Regularization term.\n",
    "        reg_loss = 0\n",
    "        if lbd > 0:\n",
    "            for user_index in range(self.user_dim):\n",
    "                reg_loss += np.sum(self.U[user_index]**2) + (0 if not self.biased else self.user_biases[user_index]**2)\n",
    "            for item_index in range(self.item_dim):\n",
    "                reg_loss += np.sum(self.V[item_index]**2) + (0 if not self.biased else self.item_biases[item_index]**2)\n",
    "            reg_loss *= lbd\n",
    "        return main_loss + reg_loss, main_loss\n",
    "\n",
    "    def train(self, lbd, iterations=20, verbose=True):\n",
    "        \"\"\"\n",
    "        Trains the recommendation system.\n",
    "\n",
    "        Keyword arguments:\n",
    "        lbd: Regularization factor.\n",
    "        iterations: Number of iterations to run ALS.\n",
    "        verbose: Whether to plot and output training loss.\n",
    "        \"\"\"\n",
    "        if self.U is None or self.V is None:\n",
    "            self.U = np.random.normal(size=(self.user_dim, self.latent_dimension))\n",
    "            self.V = np.random.normal(size=(self.item_dim, self.latent_dimension))\n",
    "            self.user_biases = np.zeros(self.user_dim)\n",
    "            self.item_biases = np.zeros(self.item_dim)\n",
    "            self.history_losses = []\n",
    "            self.history_main_losses = []\n",
    "            self.history_avg_score = []\n",
    "            self.history_avg_rank = []\n",
    "\n",
    "        it = 0\n",
    "        while(it < iterations):\n",
    "            self._single_step(lbd)\n",
    "            loss, main_loss = self.compute_loss(lbd)\n",
    "            self.history_losses.append(loss)\n",
    "            self.history_main_losses.append(main_loss)\n",
    "\n",
    "            if verbose:\n",
    "                clear_output(wait=True)\n",
    "                print('LOSS:', loss, 'MAIN LOSS:', main_loss)\n",
    "\n",
    "                plt.figure(figsize=(5,5))\n",
    "                plt.title('training loss (lower is better)')\n",
    "                plt.plot(range(len(self.history_losses)), self.history_losses)\n",
    "                plt.plot(range(len(self.history_main_losses)), self.history_main_losses, color='orange')\n",
    "                plt.plot(range(len(self.history_main_losses)), np.array(self.history_losses) - np.array(self.history_main_losses), color='green')\n",
    "                plt.legend(['total loss', 'data loss', 'regularizing loss'])\n",
    "                if self.log_dir is not None:\n",
    "                    plt.savefig(os.path.join(self.log_dir, 'log.png'), bbox_inches='tight', format='png')\n",
    "                plt.show()\n",
    "            it += 1\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Resets the recommendation system's internal state.\"\"\"\n",
    "        self.U = None\n",
    "        self.V = None\n",
    "\n",
    "    def score(self, user_id, items_id):\n",
    "        \"\"\"Returns the scoring of item_id for user_id.\"\"\"\n",
    "        if self.U is None or self.V is None:\n",
    "            raise ValueError('system has to be trained first')\n",
    "        if user_id not in self.mapping_users:\n",
    "            raise ValueError('user unknown')\n",
    "        if items_id not in self.mapping_items:\n",
    "            raise ValueError('item unknown')\n",
    "\n",
    "        user_index = self.mapping_users[user_id]\n",
    "        items_index = self.mapping_items[items_id]\n",
    "        pred = np.dot(self.U[user_index], self.V[items_index])\n",
    "        if self.biased: # Include applicable biases.\n",
    "            pred += self.user_biases[user_index] + self.item_biases[items_index]\n",
    "        return pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>items</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>spotify:track:0GO8y8jQk1PkHzS31d699N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>spotify:track:0MKqeOVdZcUFGJvWpGCKbG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>spotify:track:0q6LuUqGLUiCPP1cbdwFs3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>spotify:track:1MoDk4XDqdX25GXaRfXY2O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>spotify:track:1UOuZs8BPGMM6Ls0jP6BjQ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user                                 items\n",
       "0    8  spotify:track:0GO8y8jQk1PkHzS31d699N\n",
       "1    8  spotify:track:0MKqeOVdZcUFGJvWpGCKbG\n",
       "2    8  spotify:track:0q6LuUqGLUiCPP1cbdwFs3\n",
       "3    8  spotify:track:1MoDk4XDqdX25GXaRfXY2O\n",
       "4    8  spotify:track:1UOuZs8BPGMM6Ls0jP6BjQ"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = []\n",
    "for i, row in co_mat.iterrows():\n",
    "    for track in row[row == 1].index.values:\n",
    "        res.append((i, track))\n",
    "res = pd.DataFrame(np.array(res), columns=['user', 'items'])\n",
    "res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(res, open(\"res_10k.pickle\", \"wb\" ), protocol=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(271516, 2)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>items</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>271511</th>\n",
       "      <td>271305</td>\n",
       "      <td>spotify:track:2bbF0vktxLmFYOrYCeYBiF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271512</th>\n",
       "      <td>271305</td>\n",
       "      <td>spotify:track:4qdUS2V7Bh93UWhObEwSnQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271513</th>\n",
       "      <td>271305</td>\n",
       "      <td>spotify:track:6M7R9BK3Etvt0UNhbHoQlR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271514</th>\n",
       "      <td>271305</td>\n",
       "      <td>spotify:track:78IxKAvzvPUxp30Skp28Qy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271515</th>\n",
       "      <td>271305</td>\n",
       "      <td>spotify:track:7GJClzimvMSghjcrKxuf1M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          user                                 items\n",
       "271511  271305  spotify:track:2bbF0vktxLmFYOrYCeYBiF\n",
       "271512  271305  spotify:track:4qdUS2V7Bh93UWhObEwSnQ\n",
       "271513  271305  spotify:track:6M7R9BK3Etvt0UNhbHoQlR\n",
       "271514  271305  spotify:track:78IxKAvzvPUxp30Skp28Qy\n",
       "271515  271305  spotify:track:7GJClzimvMSghjcrKxuf1M"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = ALSRecommenderSystem(res, True, latent_dimension=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/10000 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-6ea07a1379ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-c9b1fcd2dfb2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, lbd, iterations, verbose)\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mwhile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_single_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlbd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlbd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-c9b1fcd2dfb2>\u001b[0m in \u001b[0;36m_single_step\u001b[0;34m(self, lbd)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC_users\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muser_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mP_users\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muser_index\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbiased\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem_biases\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV_exp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV_exp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlbd\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV_exp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m             \u001b[0mU_exp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muser_index\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbiased\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rs.train(0.01, iterations=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def als_similar_songs_playlist(model, orig_df, target_playlist_id, cand_list_size):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    model: the recommendation system that was trained on the training set with latent factors\n",
    "    orig_df: original df with tracks as rows, but with playlistid and other features (e.g., train)\n",
    "    target_playlist_id: id of the target playlist\n",
    "    target_playlist_inx: index of playlist in the training set\n",
    "    cand_list_size: candidate list of songs to recommend size (= test-set size * 15)\n",
    "    \n",
    "    Output:\n",
    "    k_song_to_recommend: the most similar tracks per track\n",
    "    \"\"\"\n",
    "    target_track_inx = np.where(train[\"Playlistid\"] == target_playlist_id)[0] # index of tracks in training playlist of target playlist\n",
    "    score_allsongs = list(map(lambda x: model.score(str(target_playlist_id), x), orig_df[\"Track_uri\"]))\n",
    "    rec_inx = np.argsort(score_allsongs)[::-1]\n",
    "    \n",
    "    cand_list = orig_df.iloc[rec_inx]['Track_uri']\n",
    "    unique_cand_list = cand_list.drop_duplicates()#list(set(cand_list)) # drop duplciated tracks\n",
    "    \n",
    "    tracks_in_target_playlist = orig_df.loc[orig_df[\"Playlistid\"] == target_playlist_id, \"Track_uri\"]\n",
    "    \n",
    "    cand_list2 = unique_cand_list.loc[~unique_cand_list.isin(tracks_in_target_playlist)] # remove songs that are in the \n",
    "    cand_list3 = cand_list2[:cand_list_size]\n",
    "    return list(cand_list3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nholdout(playlist_id, df):\n",
    "    '''Pass in a playlist id to get number of songs held out in val/test set'''\n",
    "    return len(df[df.Playlistid == playlist_id].Track_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_precision(prediction, val_set):\n",
    "    # prediction should be a list of predictions\n",
    "    # val_set should be pandas Series of ground truths\n",
    "    score = np.sum(val_set.isin(prediction))/val_set.shape[0]\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NDCG Code Source: https://gist.github.com/bwhite/3726239\n",
    "def dcg_at_k(r, k, method=0):\n",
    "    r = np.asfarray(r)[:k]\n",
    "    if r.size:\n",
    "        if method == 0:\n",
    "            return r[0] + np.sum(r[1:] / np.log2(np.arange(2, r.size + 1)))\n",
    "        elif method == 1:\n",
    "            return np.sum(r / np.log2(np.arange(2, r.size + 2)))\n",
    "        else:\n",
    "            raise ValueError('method must be 0 or 1.')\n",
    "    return 0.\n",
    "\n",
    "\n",
    "def ndcg_at_k(r, k, method=0):\n",
    "    dcg_max = dcg_at_k(sorted(r, reverse=True), k, method)\n",
    "    if not dcg_max:\n",
    "        return 0.\n",
    "    return dcg_at_k(r, k, method) / dcg_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rps = []\n",
    "ndcgs = []\n",
    "for pid in co_mat.index:\n",
    "    ps = als_similar_songs_playlist(rs, train, pid, nholdout(pid, train)*15)\n",
    "    vs = test[test.Playlistid == pid].Track_uri # ground truth\n",
    "    rps.append(r_precision(ps, vs))\n",
    "    \n",
    "    r = np.zeros(len(ps))\n",
    "    for i, p in enumerate(ps):\n",
    "        if np.any(vs.isin([p])):\n",
    "            r[i] = 1\n",
    "    ndcgs.append(ndcg_at_k(r, len(r)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. R-Precision:  0.23969273359366242\n",
      "Avg. NDCG:  0.1504029917610824\n",
      "Total Sum:  0.1950478626773724\n"
     ]
    }
   ],
   "source": [
    "avg_rp = np.mean(rps)\n",
    "avg_ndcg = np.mean(ndcgs)\n",
    "print('Avg. R-Precision: ', avg_rp)\n",
    "print('Avg. NDCG: ', avg_ndcg)\n",
    "print('Total Sum: ', np.mean([avg_rp, avg_ndcg]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
